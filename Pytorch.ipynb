{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdrLvIx_2eH6"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sH7Lx7g3dKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f8bf33-9ba2-4d7f-986e-b74180d8b957"
      },
      "source": [
        "#With random or constant values:\n",
        "#shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.8725, 0.8792, 0.9792],\n",
            "        [0.9985, 0.2746, 0.9548]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWifa-ty3wEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2177050f-1e51-4a47-fc7e-f718645f8954"
      },
      "source": [
        "#Tensor attributes \n",
        "tensor = torch.rand(3,4)\n",
        "print(tensor.shape)\n",
        "print(tensor.dtype)\n",
        "print(tensor.device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.float32\n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LeI-eAx4CyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9909989c-506e-4c96-da92-e1f2fcd9a441"
      },
      "source": [
        "#Tensor Operations\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "\n",
        "#Standard numpy-like indexing and slicing:\n",
        "tensor = torch.ones(4, 4)\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "\n",
        "#Join tensor\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew9fOTN54wCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12bb61c-868f-4cf8-a2ac-1347718b720d"
      },
      "source": [
        "#Multiplying tensors\n",
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tjUJMCC45XX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d8c68d-3068-429a-865d-4d791e9fc81c"
      },
      "source": [
        "#This computes the matrix multiplication between two tensors\n",
        "\n",
        "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor.matmul(tensor.T) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor @ tensor.T \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCBvGGPx5HNc"
      },
      "source": [
        "TORCH.AUTOGRAD\n",
        "\n",
        "torch.autograd tracks operations on all tensors which have their requires_grad flag set to True. For tensors that don’t require gradients, setting this attribute to False excludes it from the gradient computation DAG.\n",
        "\n",
        "The output tensor of an operation will require gradients even if only a single input tensor has requires_grad=True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ6SuFUc5G28"
      },
      "source": [
        "'''TORCH.AUTOGRAD\n",
        "torch.autograd is PyTorch’s automatic differentiation engine that powers neural \n",
        "network training. In this section, you will get a conceptual understanding of \n",
        "how autograd helps a neural network train. '''\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ORvPvNJ8NVC"
      },
      "source": [
        "$$\n",
        "Q = 3a^3 - b^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkmM-7M7zGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd73314-6259-4f7f-aef5-6109d4553221"
      },
      "source": [
        "Q = 3*a**3 - b**2\n",
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)\n",
        "\n",
        "#Gradients are now deposited in a.grad and b.grad\n",
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5pCN72f_V65"
      },
      "source": [
        "Neural Network\n",
        "\n",
        "Neural networks can be constructed using the torch.nn package.\n",
        "\n",
        "*Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input) that returns the output.*\n",
        "\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4scHICijgNOl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQi8c3vwgOZe"
      },
      "source": [
        "Let’s understand PyTorch through a more practical lens. Learning theory is good, but it isn’t much use if you don’t put it into practice!\n",
        "\n",
        "A PyTorch implementation of a neural network looks exactly like a NumPy implementation. The goal of this section is to showcase the equivalent nature of PyTorch and NumPy. For this purpose, let’s create a simple three-layered network having 5 nodes in the input layer, 3 in the hidden layer, and 1 in the output layer. We will use only one training example with one row which has five features and one target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-QfUDpsCTlC"
      },
      "source": [
        "#Basic Neural net\n",
        "n_input , n_hidden , n_output = 5 , 3 , 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si7yPKl8gTsv"
      },
      "source": [
        "The first step is to do parameter initialization. Here, the weights and bias parameters for each layer are initialized as the tensor variables. Tensors are the base data structures of PyTorch which are used for building different types of neural networks. They can be considered as the generalization of arrays and matrices; in other words, tensors are N-dimensional matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ8-ZmJplgnf",
        "outputId": "66417a25-0d13-4ef9-e191-618d23ca415c"
      },
      "source": [
        "## initialize tensor for inputs, and outputs \n",
        "x = torch.randn((1, n_input))\n",
        "y = torch.randn((1, n_output))\n",
        "print(x)\n",
        "print(y)\n",
        "## initialize tensor variables for weights \n",
        "w1 = torch.randn(n_input, n_hidden) # weight for hidden layer\n",
        "w2 = torch.randn(n_hidden, n_output) # weight for output layer\n",
        "print(w1)\n",
        "print(w2)\n",
        "## initialize tensor variables for bias terms \n",
        "b1 = torch.randn((1, n_hidden)) # bias for hidden layer\n",
        "b2 = torch.randn((1, n_output)) # bias for output layer\n",
        "print(b1)\n",
        "print(b2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0110,  0.4247,  0.6480,  3.1905, -0.1734]])\n",
            "tensor([[-0.7587]])\n",
            "tensor([[-2.5228,  0.4819,  0.0272],\n",
            "        [ 0.6909, -1.7849, -0.2146],\n",
            "        [ 0.7728,  1.4393,  1.0957],\n",
            "        [ 0.4781,  1.6440,  0.8901],\n",
            "        [-0.4864, -0.1480, -1.3468]])\n",
            "tensor([[ 1.2191],\n",
            "        [-0.7388],\n",
            "        [ 2.2757]])\n",
            "tensor([[-0.1492, -2.5270,  0.5076]])\n",
            "tensor([[2.2772]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFlCngSHl9P4"
      },
      "source": [
        "After the parameter initialization step, a neural network can be defined and trained in four key steps:\n",
        "\n",
        "- Forward Propagation\n",
        "- Loss computation\n",
        "- Backpropagation\n",
        "- Updating the parameters\n",
        "\n",
        "Let’s see each of these steps in a bit more detail.\n",
        "\n",
        "***Forward Propagation:*** In this step, activations are calculated at every layer using the two steps shown below. These activations flow in the forward direction from the input layer to the output layer in order to generate the final output.\n",
        "\n",
        "1. *z = weight * input + bias*\n",
        "2. *a = activation_function (z)*\n",
        "\n",
        "The following code blocks show how we can write these steps in PyTorch. Notice that most of the functions, such as exponential and matrix multiplication, are similar to the ones in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYOInWf08vsB"
      },
      "source": [
        "#Sigmoid function using pytorch\n",
        "\n",
        "def sigmoid_activation(z):\n",
        "    return 1 / (1 + torch.exp(-z))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzm8ZBppmVdD",
        "outputId": "838e5d38-c03f-4071-f693-b61bcc40d488"
      },
      "source": [
        "#activation of hidden layers\n",
        "#https://pytorch.org/docs/stable/generated/torch.mm.html\n",
        "z1 = torch.mm(x , w1) + b1\n",
        "a1 = sigmoid_activation(z1)\n",
        "\n",
        "print(z1)\n",
        "print(a1)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.2795, 2.9123, 4.1975]])\n",
            "tensor([[0.9072, 0.9485, 0.9852]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haRKT3UInxzo",
        "outputId": "081c9585-d686-47fe-8b3a-d2c9a12a2501"
      },
      "source": [
        "#Activation for out put layer\n",
        "z2 = torch.mm(a1 , w2) + b2\n",
        "output = sigmoid_activation(z2)\n",
        "\n",
        "print(z2)\n",
        "print(output)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.9210]])\n",
            "tensor([[0.9928]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAnT5xHinxI_"
      },
      "source": [
        "\n",
        "**Loss Computation:** In this step, the error (also called loss) is calculated in the output layer. A simple loss function can tell the difference between the actual value and the predicted value. Later, we will look at different loss functions available in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb83QaeSo7SZ",
        "outputId": "b96369d7-3e50-49d0-9037-654a2eff0196"
      },
      "source": [
        "loss = y - output\n",
        "print(loss)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.7514]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkFgKCpHpEa2"
      },
      "source": [
        "**Backpropagation**: The aim of this step is to minimize the error in the output layer by making marginal changes in the bias and the weights. These marginal changes are computed using the derivatives of the error term.\n",
        "\n",
        "Based on the Calculus principle of the Chain rule, the delta changes are back passed to hidden layers where corresponding changes in their weights and bias are made. This leads to an adjustment in the weights and bias until the error is minimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ytQCETpFC0"
      },
      "source": [
        "#Function to calculate the derivative\n",
        "def sigmoid_delta(x):\n",
        "  return x*(1-x)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jvFPogwpT7a",
        "outputId": "0a217314-acf3-430f-d674-84d6a9b00fec"
      },
      "source": [
        "#Computes derivative of error terms\n",
        "delta_output = sigmoid_delta(output)\n",
        "delta_hidden =  sigmoid_delta(a1)\n",
        "\n",
        "print(delta_output)\n",
        "print(delta_hidden)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0072]])\n",
            "tensor([[0.0842, 0.0489, 0.0146]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMKgCjfxpyW3",
        "outputId": "e2541a0b-6619-4596-82f1-3d9238a79abe"
      },
      "source": [
        "##backpass the changes to previous layers\n",
        "d_outp = loss * delta_output\n",
        "loss_h = torch.mm(d_outp, w2.t())\n",
        "d_hidn = loss_h*delta_hidden\n",
        "\n",
        "print(d_outp)\n",
        "print(loss_h)\n",
        "print(d_hidn)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0126]])\n",
            "tensor([[-0.0153,  0.0093, -0.0286]])\n",
            "tensor([[-0.0013,  0.0005, -0.0004]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgwHNbsFrAGl"
      },
      "source": [
        "Updating the Parameters: Finally, the weights and bias are updated using the delta changes received from the above backpropagation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm_lJrbdrBd3"
      },
      "source": [
        "learning_rate = 0.1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5or83EcnrD_z",
        "outputId": "59765cbd-0db3-4605-e0ae-73762ef9a1fe"
      },
      "source": [
        "w2 += torch.mm(a1.t(), d_outp) * learning_rate\n",
        "w1 += torch.mm(x.t(), d_hidn) * learning_rate\n",
        "\n",
        "print(w2)\n",
        "print(w1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2168],\n",
            "        [-0.7412],\n",
            "        [ 2.2732]])\n",
            "tensor([[-2.5228,  0.4819,  0.0272],\n",
            "        [ 0.6907, -1.7848, -0.2147],\n",
            "        [ 0.7726,  1.4393,  1.0956],\n",
            "        [ 0.4772,  1.6443,  0.8898],\n",
            "        [-0.4864, -0.1480, -1.3468]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdk6U5n1rZeo",
        "outputId": "de396326-6067-43ba-86b9-b59be2775a6a"
      },
      "source": [
        "b2 +=d_outp.sum()*learning_rate\n",
        "b1 +=d_hidn.sum()*learning_rate\n",
        "print(b2)\n",
        "print(b1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.2760]])\n",
            "tensor([[-0.1521, -2.5299,  0.5047]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7QcBgolrx9o"
      },
      "source": [
        "Finally, when these steps are executed for a number of epochs with a large number of training examples, the loss is reduced to a minimum value. The final weight and bias values are obtained which can then be used to make predictions on the unseen data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfAFGvwOviQq"
      },
      "source": [
        "**ConvNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyqlaaY1rwdr",
        "outputId": "14b04dad-d57a-45ce-8520-5a268e60d889"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    #1 inpuut image channel , 6 output,3x3 square Convolution kernel\n",
        "    self.conv1 = nn.Conv2d(1,6,3)\n",
        "    self.conv2 = nn.Conv2d(6,16,3)\n",
        "\n",
        "    self.fc1 = nn.Linear(16 * 6 * 6 , 120)\n",
        "    self.fc2 = nn.Linear(120 , 84)\n",
        "    self.fc3 = nn.Linear(84 , 10)\n",
        "\n",
        "    def forward(self,x):\n",
        "      #Max pooling over (2,2) window\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
        "      x = x.view(-1 , self.num_flat_features(x))\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "    \n",
        "    def num_flat_features(self ,x):\n",
        "      size = x.size()[1:]\n",
        "      num_features = 1\n",
        "      for s in size:\n",
        "        num_features *= s\n",
        "      return num_features\n",
        "\n",
        "net = Net()\n",
        "print(net)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcxv_GJJrqJ2",
        "outputId": "3873eefb-2b7f-46f7-e25f-8724ce0abbdf"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blQrVdhoy6zs"
      },
      "source": [
        "Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAceS00t2Ko_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOGIDxP10hoo"
      },
      "source": [
        "- torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
        "- nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
        "- nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
        "- autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSfZ3neD0sWF"
      },
      "source": [
        "'''\n",
        "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "      -> view -> linear -> relu -> linear -> relu -> linear\n",
        "      -> MSELoss\n",
        "      -> loss\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}