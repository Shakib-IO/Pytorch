{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdrLvIx_2eH6"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sH7Lx7g3dKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0156855e-fddb-495d-969a-1568d2e4d005"
      },
      "source": [
        "#With random or constant values:\n",
        "#shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.5572, 0.0545, 0.0020],\n",
            "        [0.5545, 0.4033, 0.9121]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWifa-ty3wEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e61308-76be-45b4-8e53-01f5cef7fd87"
      },
      "source": [
        "#Tensor attributes \n",
        "tensor = torch.rand(3,4)\n",
        "print(tensor.shape)\n",
        "print(tensor.dtype)\n",
        "print(tensor.device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.float32\n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LeI-eAx4CyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ceb842b-7947-4d5d-cdd5-ee6798dffe45"
      },
      "source": [
        "#Tensor Operations\n",
        "# https://pytorch.org/docs/stable/torch.html\n",
        "\n",
        "#Standard numpy-like indexing and slicing:\n",
        "tensor = torch.ones(4, 4)\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "\n",
        "#Join tensor\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew9fOTN54wCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e8e81bb-ccb4-4d9d-ab88-2cc9c870fa6c"
      },
      "source": [
        "#Multiplying tensors\n",
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tjUJMCC45XX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99ddbb3-fbd8-4126-ab0b-c7d54f4cbf60"
      },
      "source": [
        "#This computes the matrix multiplication between two tensors\n",
        "\n",
        "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor.matmul(tensor.T) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor @ tensor.T \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCBvGGPx5HNc"
      },
      "source": [
        "TORCH.AUTOGRAD\n",
        "\n",
        "torch.autograd tracks operations on all tensors which have their requires_grad flag set to True. For tensors that don’t require gradients, setting this attribute to False excludes it from the gradient computation DAG.\n",
        "\n",
        "The output tensor of an operation will require gradients even if only a single input tensor has requires_grad=True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ6SuFUc5G28"
      },
      "source": [
        "'''TORCH.AUTOGRAD\n",
        "torch.autograd is PyTorch’s automatic differentiation engine that powers neural \n",
        "network training. In this section, you will get a conceptual understanding of \n",
        "how autograd helps a neural network train. '''\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ORvPvNJ8NVC"
      },
      "source": [
        "$$\n",
        "Q = 3a^3 - b^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkmM-7M7zGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9a6f28-7f1e-4435-9546-84001cb1c1c2"
      },
      "source": [
        "Q = 3*a**3 - b**2\n",
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)\n",
        "\n",
        "#Gradients are now deposited in a.grad and b.grad\n",
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5pCN72f_V65"
      },
      "source": [
        "Neural Network\n",
        "\n",
        "Neural networks can be constructed using the torch.nn package.\n",
        "\n",
        "*Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input) that returns the output.*\n",
        "\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4scHICijgNOl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQi8c3vwgOZe"
      },
      "source": [
        "Let’s understand PyTorch through a more practical lens. Learning theory is good, but it isn’t much use if you don’t put it into practice!\n",
        "\n",
        "A PyTorch implementation of a neural network looks exactly like a NumPy implementation. The goal of this section is to showcase the equivalent nature of PyTorch and NumPy. For this purpose, let’s create a simple three-layered network having 5 nodes in the input layer, 3 in the hidden layer, and 1 in the output layer. We will use only one training example with one row which has five features and one target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-QfUDpsCTlC"
      },
      "source": [
        "#Basic Neural net\n",
        "n_input , n_hidden , n_output = 5 , 3 , 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si7yPKl8gTsv"
      },
      "source": [
        "The first step is to do parameter initialization. Here, the weights and bias parameters for each layer are initialized as the tensor variables. Tensors are the base data structures of PyTorch which are used for building different types of neural networks. They can be considered as the generalization of arrays and matrices; in other words, tensors are N-dimensional matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ8-ZmJplgnf",
        "outputId": "91c4f5c3-9cc4-41c0-e88d-2c9ed7d37d81"
      },
      "source": [
        "## initialize tensor for inputs, and outputs \n",
        "x = torch.randn((1, n_input))\n",
        "y = torch.randn((1, n_output))\n",
        "print(x)\n",
        "print(y)\n",
        "## initialize tensor variables for weights \n",
        "w1 = torch.randn(n_input, n_hidden) # weight for hidden layer\n",
        "w2 = torch.randn(n_hidden, n_output) # weight for output layer\n",
        "print(w1)\n",
        "print(w2)\n",
        "## initialize tensor variables for bias terms \n",
        "b1 = torch.randn((1, n_hidden)) # bias for hidden layer\n",
        "b2 = torch.randn((1, n_output)) # bias for output layer\n",
        "print(b1)\n",
        "print(b2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.3645,  1.2006, -0.1160, -1.4808, -0.3872]])\n",
            "tensor([[-0.7573]])\n",
            "tensor([[-0.0557,  0.3002,  1.5590],\n",
            "        [ 0.1163,  0.0174,  1.3291],\n",
            "        [-0.1112,  0.5747, -1.6353],\n",
            "        [-2.2377, -1.3319, -1.8946],\n",
            "        [ 0.2566, -0.3503,  0.9630]])\n",
            "tensor([[-0.4073],\n",
            "        [ 0.3621],\n",
            "        [ 0.6168]])\n",
            "tensor([[-0.7392, -0.5973, -0.8756]])\n",
            "tensor([[-1.5617]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFlCngSHl9P4"
      },
      "source": [
        "After the parameter initialization step, a neural network can be defined and trained in four key steps:\n",
        "\n",
        "- Forward Propagation\n",
        "- Loss computation\n",
        "- Backpropagation\n",
        "- Updating the parameters\n",
        "\n",
        "Let’s see each of these steps in a bit more detail.\n",
        "\n",
        "***Forward Propagation:*** In this step, activations are calculated at every layer using the two steps shown below. These activations flow in the forward direction from the input layer to the output layer in order to generate the final output.\n",
        "\n",
        "1. *z = weight * input + bias*\n",
        "2. *a = activation_function (z)*\n",
        "\n",
        "The following code blocks show how we can write these steps in PyTorch. Notice that most of the functions, such as exponential and matrix multiplication, are similar to the ones in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYOInWf08vsB"
      },
      "source": [
        "#Sigmoid function using pytorch\n",
        "\n",
        "def sigmoid_activation(z):\n",
        "    return 1 / (1 + torch.exp(-z))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzm8ZBppmVdD",
        "outputId": "f6ed0b47-efe6-4af2-a1ad-9a3b841a9757"
      },
      "source": [
        "#activation of hidden layers\n",
        "#https://pytorch.org/docs/stable/generated/torch.mm.html\n",
        "z1 = torch.mm(x , w1) + b1\n",
        "a1 = sigmoid_activation(z1)\n",
        "\n",
        "print(z1)\n",
        "print(a1)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.5516, 1.8745, 5.4696]])\n",
            "tensor([[0.9277, 0.8670, 0.9958]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haRKT3UInxzo",
        "outputId": "ab9eed68-0175-4751-aaac-b6d4acca6f8f"
      },
      "source": [
        "#Activation for out put layer\n",
        "z2 = torch.mm(a1 , w2) + b2\n",
        "output = sigmoid_activation(z2)\n",
        "\n",
        "print(z2)\n",
        "print(output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0115]])\n",
            "tensor([[0.2667]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAnT5xHinxI_"
      },
      "source": [
        "\n",
        "**Loss Computation:** In this step, the error (also called loss) is calculated in the output layer. A simple loss function can tell the difference between the actual value and the predicted value. Later, we will look at different loss functions available in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb83QaeSo7SZ",
        "outputId": "f0c678f3-8e6e-4aa5-921e-1bf47032ff16"
      },
      "source": [
        "loss = y - output\n",
        "print(loss)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0240]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkFgKCpHpEa2"
      },
      "source": [
        "**Backpropagation**: The aim of this step is to minimize the error in the output layer by making marginal changes in the bias and the weights. These marginal changes are computed using the derivatives of the error term.\n",
        "\n",
        "Based on the Calculus principle of the Chain rule, the delta changes are back passed to hidden layers where corresponding changes in their weights and bias are made. This leads to an adjustment in the weights and bias until the error is minimized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ytQCETpFC0"
      },
      "source": [
        "#Function to calculate the derivative\n",
        "def sigmoid_delta(x):\n",
        "  return x*(1-x)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jvFPogwpT7a",
        "outputId": "9c0f1287-8781-4090-9fc4-0448b64053bd"
      },
      "source": [
        "#Computes derivative of error terms\n",
        "delta_output = sigmoid_delta(output)\n",
        "delta_hidden =  sigmoid_delta(a1)\n",
        "\n",
        "print(delta_output)\n",
        "print(delta_hidden)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1956]])\n",
            "tensor([[0.0671, 0.1153, 0.0042]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMKgCjfxpyW3",
        "outputId": "6ce2dbef-6fd4-47a3-f5d4-4f240da6a93c"
      },
      "source": [
        "##backpass the changes to previous layers\n",
        "d_outp = loss * delta_output\n",
        "loss_h = torch.mm(d_outp, w2.t())\n",
        "d_hidn = loss_h*delta_hidden\n",
        "\n",
        "print(d_outp)\n",
        "print(loss_h)\n",
        "print(d_hidn)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2003]])\n",
            "tensor([[ 0.0816, -0.0725, -0.1235]])\n",
            "tensor([[ 0.0055, -0.0084, -0.0005]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgwHNbsFrAGl"
      },
      "source": [
        "Updating the Parameters: Finally, the weights and bias are updated using the delta changes received from the above backpropagation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm_lJrbdrBd3"
      },
      "source": [
        "learning_rate = 0.1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5or83EcnrD_z",
        "outputId": "c9e346e6-2fb5-4928-d20e-5783e066ed79"
      },
      "source": [
        "w2 += torch.mm(a1.t(), d_outp) * learning_rate\n",
        "w1 += torch.mm(x.t(), d_hidn) * learning_rate\n",
        "\n",
        "print(w2)\n",
        "print(w1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.4259],\n",
            "        [ 0.3447],\n",
            "        [ 0.5968]])\n",
            "tensor([[-0.0549,  0.2991,  1.5590],\n",
            "        [ 0.1170,  0.0164,  1.3290],\n",
            "        [-0.1113,  0.5748, -1.6353],\n",
            "        [-2.2385, -1.3307, -1.8945],\n",
            "        [ 0.2564, -0.3500,  0.9630]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdk6U5n1rZeo",
        "outputId": "6ba1fd55-77e0-46a5-8424-ce94c00beb68"
      },
      "source": [
        "b2 +=d_outp.sum()*learning_rate\n",
        "b1 +=d_hidn.sum()*learning_rate\n",
        "print(b2)\n",
        "print(b1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.5817]])\n",
            "tensor([[-0.7395, -0.5977, -0.8759]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7QcBgolrx9o"
      },
      "source": [
        "Finally, when these steps are executed for a number of epochs with a large number of training examples, the loss is reduced to a minimum value. The final weight and bias values are obtained which can then be used to make predictions on the unseen data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfAFGvwOviQq"
      },
      "source": [
        "**ConvNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyqlaaY1rwdr",
        "outputId": "0b0aeee0-105f-4581-855b-8f3b03275a16"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    #1 inpuut image channel , 6 output,3x3 square Convolution kernel\n",
        "    self.conv1 = nn.Conv2d(1,6,3)\n",
        "    self.conv2 = nn.Conv2d(6,16,3)\n",
        "\n",
        "    self.fc1 = nn.Linear(16 * 6 * 6 , 120)\n",
        "    self.fc2 = nn.Linear(120 , 84)\n",
        "    self.fc3 = nn.Linear(84 , 10)\n",
        "\n",
        "    def forward(self,x):\n",
        "      #Max pooling over (2,2) window\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
        "      x = x.view(-1 , self.num_flat_features(x))\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "    \n",
        "    def num_flat_features(self ,x):\n",
        "      size = x.size()[1:]\n",
        "      num_features = 1\n",
        "      for s in size:\n",
        "        num_features *= s\n",
        "      return num_features\n",
        "\n",
        "net = Net()\n",
        "print(net)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcxv_GJJrqJ2",
        "outputId": "d55ec9a7-a224-4748-8c1d-1c05e28096a4"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blQrVdhoy6zs"
      },
      "source": [
        "Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAceS00t2Ko_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOGIDxP10hoo"
      },
      "source": [
        "- torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
        "- nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
        "- nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
        "- autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSfZ3neD0sWF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "90c85608-91f8-41bf-bfbb-59dd88c28ac7"
      },
      "source": [
        "'''\n",
        "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "      -> view -> linear -> relu -> linear -> relu -> linear\n",
        "      -> MSELoss\n",
        "      -> loss\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\\n      -> view -> linear -> relu -> linear -> relu -> linear\\n      -> MSELoss\\n      -> loss\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oboclryp-W9v"
      },
      "source": [
        "Simple Neural Network\n",
        "At its core, PyTorch provides two main features:\n",
        "\n",
        "- An n-dimensional Tensor, similar to numpy but can run on GPUs\n",
        "- Automatic differentiation for building and training neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaSY79ug-Wqu",
        "outputId": "a11a8698-aa08-4f79-f04c-69fde27a112d"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Using Numpy\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Create random input and output data\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 293.10181301084606\n",
            "199 199.9850253756452\n",
            "299 137.48055379370865\n",
            "399 95.49062710676895\n",
            "499 67.25855462373715\n",
            "599 48.26016518234365\n",
            "699 35.46400309849854\n",
            "799 26.837320119857818\n",
            "899 21.01600694947048\n",
            "999 17.083925448170366\n",
            "1099 14.425286600108127\n",
            "1199 12.625828575036008\n",
            "1299 11.406616679636533\n",
            "1399 10.579664210261377\n",
            "1499 10.018159061343574\n",
            "1599 9.636473420996552\n",
            "1699 9.376731011666974\n",
            "1799 9.19977311959191\n",
            "1899 9.079077630996775\n",
            "1999 8.99666209842717\n",
            "Result: y = -0.010380849961281679 + 0.8478679324111694 x + 0.0017908693564100595 x^2 + -0.09206829476482772 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnznLvCK_BTR"
      },
      "source": [
        "PyTorch: Tensors\n",
        "\n",
        "\n",
        "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
        "\n",
        "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing.\n",
        "\n",
        "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to specify the correct device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DipBacS_GkV",
        "outputId": "64beae63-06c6-48d6-e6ec-2ec7cfffcbf4"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "b = torch.randn((), device=device, dtype=dtype)\n",
        "c = torch.randn((), device=device, dtype=dtype)\n",
        "d = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 902.4918823242188\n",
            "199 614.76318359375\n",
            "299 420.1158752441406\n",
            "399 288.30194091796875\n",
            "499 198.94354248046875\n",
            "599 138.30067443847656\n",
            "699 97.1005859375\n",
            "799 69.07864379882812\n",
            "899 49.998409271240234\n",
            "999 36.99180603027344\n",
            "1099 28.115312576293945\n",
            "1199 22.05055809020996\n",
            "1299 17.902122497558594\n",
            "1399 15.061223030090332\n",
            "1499 13.113500595092773\n",
            "1599 11.776597023010254\n",
            "1699 10.857935905456543\n",
            "1799 10.225961685180664\n",
            "1899 9.790727615356445\n",
            "1999 9.490652084350586\n",
            "Result: y = 0.02253175526857376 + 0.8711710572242737 x + -0.003887103172019124 x^2 + -0.09538295865058899 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds94FzCr_VG_"
      },
      "source": [
        "Autograd\n",
        "\n",
        "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
        "\n",
        "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXWUqC3__Uo8",
        "outputId": "fb1d184b-2e64-4f89-b3be-b9a3f670088f"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
        "\n",
        "# Create Tensors to hold input and outputs.\n",
        "# By default, requires_grad=False, which indicates that we do not need to\n",
        "# compute gradients with respect to these Tensors during the backward pass.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Create random Tensors for weights. For a third order polynomial, we need\n",
        "# 4 weights: y = a + b x + c x^2 + d x^3\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y using operations on Tensors.\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
        "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    with torch.no_grad():\n",
        "        a -= learning_rate * a.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        c -= learning_rate * c.grad\n",
        "        d -= learning_rate * d.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        a.grad = None\n",
        "        b.grad = None\n",
        "        c.grad = None\n",
        "        d.grad = None\n",
        "\n",
        "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 172.9054718017578\n",
            "199 120.36891174316406\n",
            "299 84.73808288574219\n",
            "399 60.54649353027344\n",
            "499 44.10340118408203\n",
            "599 32.91468048095703\n",
            "699 25.29262924194336\n",
            "799 20.094390869140625\n",
            "899 16.54515266418457\n",
            "999 14.119000434875488\n",
            "1099 12.458662033081055\n",
            "1199 11.321118354797363\n",
            "1299 10.5408296585083\n",
            "1399 10.005022048950195\n",
            "1499 9.636653900146484\n",
            "1599 9.383135795593262\n",
            "1699 9.208451271057129\n",
            "1799 9.087963104248047\n",
            "1899 9.004761695861816\n",
            "1999 8.947249412536621\n",
            "Result: y = -0.010156458243727684 + 0.862734317779541 x + 0.0017521579284220934 x^2 + -0.09418290853500366 x^3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbW1rIt8_g9E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}